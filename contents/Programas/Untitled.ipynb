{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma classification\n",
    "Melanoma is a type of skin cancer. It's the common type of skin cancer, on surfing the internet, we will come images of Melanoma which have a dark-black spot on the skin. We'll create a siamese Convolutional Neural Network with TensorFlow using this notebook to detect two classes: **Melanoma and Non-Melanoma**.\n",
    "## 1) Import the packages.\n",
    "We import the tensorFlow package along with NumPy and Keras. Also, we set the verbosity of TensorFlow to \"tf.logging.EROR\" so that only errors are being printed in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c13ea86cdcaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__VERSION__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'logging'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(tf.__VERSION__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dowloading the preprocessed data\n",
    "Processing and loading of data on a notebook could be time-consumming, so we directly download the processed from GitHub. The data was collected and processed in these steps:\n",
    "Images => \n",
    "    1. Images of Melanoma were collected from the Internet (using [Fatkun Chrome Extension](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf) . Some images of healthy skin were also collected. \n",
    "    2. Due to the lesser amount of data, the images were augmented to increase their number upto 5041. Also, they were resized to dimensions 32x32 px.\n",
    "    3. The RGB velues were normalized (brought down to the range(0,1)).\n",
    "    4. Converted to Numpy of shape (5041, 32, 32,3)\n",
    "Labels =>\n",
    "    1. Pairs of images were generated.\n",
    "    2. If both the images in the pair belongs to the same class then the label of 1 was assigned else 0 was assigned. \n",
    "    3. Converted to NumPy array of shape (5041, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-424ffe242ccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://github.com/shubham0204/Dataset_Archives/blob/master/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1223\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1226\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get('https://github.com/chunche95/IntroMachineLearning/blob/master/contents/Programas/Melanoma-Images/mel_detect_processed.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Reshaping the data\n",
    "We have total 50411 images and 5041 labels. We make a split of 4500-541 for training and validation receptively. \n",
    "    * We squeeze the last 3 dimensions of the image array. Hence the shape transform into(5041, 32, 32, 3) --> (5041, 3072)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_data/x1.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ba2bd892f09b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDIMEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'processed_data/x1.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'processed_data/x2.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'processed_data/y.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_data/x1.py'"
     ]
    }
   ],
   "source": [
    "DIMEN = 32\n",
    "X1 = np.load('processed_data/x1.py') \n",
    "X2 = np.load('processed_data/x2.py')\n",
    "Y =  np.load('processed_data/y.py')\n",
    "\n",
    "X1 = X1.reshape( (X1.shape[0], DIMEN**2*3) ).astype(np.float32)\n",
    "X2 = X2.reshape( (X2.shape[0], DIMEN**2*3) ).astype(np.float32)\n",
    "\n",
    "train_X1 = X1[ : 4500 ]\n",
    "train_X2 = X2[ : 4500 ]\n",
    "train_Y = Y[ : 4500 ]\n",
    "\n",
    "print( train_X1.shape)\n",
    "print( train_X2.shape)\n",
    "print( train_Y.shape)\n",
    "\n",
    "print( test_X1.shape)\n",
    "print( test_X2.shape)\n",
    "print( test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Defining the Model.\n",
    "We use Keras to built out Siamese Convolutional Neural Network. The layers will perform \n",
    "operations in this manner.\n",
    "    1. Reshape the input (from(None, 3072) to (None,32,32,3))\n",
    "    2. Extract features using \"Conv2D\" and \"MaxPooling2D\" layers\n",
    "    3. A \"Dense\" layer to produce a binary output using \"sigmoid\" activation function.\n",
    "    4. Triplet Loss Function\n",
    "    5.Produce a similarity score using \"sigmoid\".\n",
    "Since our is binary in nature we use \"tf.keras.losser.binary_crossentropy\" loss function with \"tf.keras.optimizers.Adam\" with a leaning rate of 0.0001.\n",
    "You can read more about these network from [here](https://medium.com/predict/face-recognition-from-scratch-using-siamese-networks-and-tensorflow-df03e32f8cd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-dca336ead693>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-dca336ead693>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    tf.keras.layers.MaxPooling2D(pool_size=pool_size_1, strides=strides),\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "input:shape = ((DIM**2)*3, )\n",
    "convolution_shape = (DIMEN, DIMEN, 3)\n",
    "\n",
    "kernel_size_1 = (8,8)\n",
    "kernel_size_2 = (6,6)\n",
    "kernel_size_3 = (4,4)\n",
    "\n",
    "pool_size_1 = (6,6)\n",
    "pool_size_2 = (4,4)\n",
    "\n",
    "strides = 1\n",
    "seq_conv_model=[\n",
    "    tf.keras.layer.Reshape( input_shape=input_shape, target_shape=convolution_shape),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=kernel_size_1, strides=strides, activation='relu')\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=pool_size_1, strides=strides),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, kernel_size=kernel_size_2, strides=strides, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=pool_size_2, strides=strides),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=kernel_size_3, strides=strides, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.dense(3076, activation=tf.keras.activations.sigmoid)\n",
    "]\n",
    "\n",
    "seq_model = tf.keras.Sequential(seq_conv_model)\n",
    "\n",
    "input_x1 = tf.keras.layers.Input(shape=input_shape)\n",
    "input_x2 = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "output_x1 = seq_model(input_x1)\n",
    "output_x2 = seq_model(input_x2)\n",
    "\n",
    "distance_euclid = tf.keras.layers.Lambda(lambda tensors: K.abs(tensors[0]-tensor[1]))([output_x1, output_x2])\n",
    "outputs = tf.keras.layers.dense(1, activation=tf.keras.activations.sigmoid) (distance_euclid)\n",
    "model = tf.keras.model.Model ([input_x1, input_x2], outputs)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
